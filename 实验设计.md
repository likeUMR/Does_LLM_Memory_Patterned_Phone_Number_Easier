核心问题：大模型记有规律的电话号码快，还是没有规律的电话号码快？

实验设计：
1. 生成3种Group的100个电话号码
    a. GroupA：随机的11位号码
    a. GroupB: 反对称的11位号码（前5位和后5位对称）
    a. GroupC: 高度重复的11位号码（前3个数字相同，中间4个数字相同，后4个数字相同）
1. 按格式组装成3个训练数据集
    a. 单条数据组装格式：第{i}个电话号码是：{xxx}
1. 对一个预训练后的LLM，分别使用3种数据集进行生成式SFT
    a. 每个数据集的SFT分别形成一个模型，相互无关
    a. 使用相同的学习率，训练相同步数，期间记录每个epoch的平均loss
    a. 训练的模型为qwen3-1.7B-FP8，使用Adam-8 bit优化器
1. 可视化训练结果
    a. 在一张折线图中绘制3条epoch-loss的折线

实验说明：
1. 使用poetry管理python环境
2. 有效显存13GB，显卡为NVIDIA 5070Ti，已安装CUDA Toolkit
3. Windows 11系统
4. 基础模型路径D:\PROJECT\VSCode\深度学习实验\Which_Phone_Number_Does_LLM_mem_Better\Qwen3-1___7B-FP8